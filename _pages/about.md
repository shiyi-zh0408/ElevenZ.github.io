---
permalink: /
title: "About Me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I'm currently a 1st-year PHD student in Tsinghua University Shenzhen International Graduate School, supervised by Prof. [Yansong Tang](https://andytang15.github.io/) and Prof. [Jiwen Lu](http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/biography.html). I got my bachelor's degree from the Department of Automation, Tsinghua University in 2023.

My research interests lie in Computer Vision, such as Video Understanding, and Embodied Visual Perception.

[Email](mailto:sy-zhang23@mails.tsinghua.edu.cn) / [Github](https://github.com/shiyi-zh0408)

---
# News
---
* 2024-03: One paper on video understanding (Narrative Action Evaluation) is accepted to [CVPR 2024](https://cvpr.thecvf.com/)
* 2023-03: One paper on video understanding (Action Quality Assessment) is accepted to [CVPR 2023](https://cvpr.thecvf.com/Conferences/2023)

---
# Research
---

<tr>
  <td style="padding:20px;width:30%;max-width:30%" align="center">
    <img style="width:100%;max-width:100%" src="images/logo.png" alt="dise">
  </td>
  <td width="75%" valign="center">
    <papertitle>LOGO: A Long-Form Video Dataset for Group Action Quality Assessment</papertitle>
    <br>
    Shiyi Zhang, Wenxun Dai, Sujia Wang, Xiangwei Shen, Jiwen Lu, Jie Zhou, <strong>Yansong Tang#</strong>
    <br>
    <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
    <br>
    <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_LOGO_A_Long-Form_Video_Dataset_for_Group_Action_Quality_Assessment_CVPR_2023_paper.pdf">[PDF]</a>
    <a href="https://github.com/shiyi-zh0408/LOGO">[Project Page]</a> 
    <br>
    <p> LOGO is a new multi-person long-form video dataset for action quality assessment.</p>
  </td>
</tr>	

<div class="row" style="text-align:center;">   
    <div class="column" style="display:inline-block;text-align:center;"> 
     	  <img src="../images/maga.png" style="margin-bottom:10px;display:block;margin-left:auto;margin-right:auto;">
    </div>
    <div class="column" style="display:inline-block;text-align:center;">    
        <p style="font-size: smaller;text-align:left;display:inline-block;">
            <b>ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation</b><br>
            Guanxing Lu, <b>Shiyi Zhang</b>, Ziwei Wang, Changliu Liu, Jiwen Lu and Yansong Tang.<br>
            <i>Preprint</i><br>
            <br>
            We propose a dynamic Gaussian Splatting method named ManiGaussian for multi-task robotic manipulation, which mines scene dynamics via future scene reconstruction.
        </p>
    </div> 
</div>

<!--Paper 1-->
<div class="row" style="text-align:center;">   
    <div class="column" style="display:inline-block;"> 
     	  <img src="../images/maga.png" style="margin-bottom:10px;">
    </div>
    <div class="column" style="display:inline-block;">    
        <p style="font-size: smaller;">
            <b>ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation</b><br>
            Guanxing Lu, <b>Shiyi Zhang</b>, Ziwei Wang, Changliu Liu, Jiwen Lu and Yansong Tang.<br>
            <i>Preprint</i><br>
            <br>
            We propose a dynamic Gaussian Splatting method named ManiGaussian for multi-task robotic manipulation, which mines scene dynamics via future scene reconstruction.
        </p>
    </div> 
</div>

<div class="row">   
    <div class="column" style="float:left;width:25%"> 
     	  <img src="../images/maga.png">
    </div>
    <div class="column" style="float:left;width:75%">    
        <b>ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation</b><br>
        Guanxing Lu, <b>Shiyi Zhang</b>, Ziwei Wang, Changliu Liu, Jiwen Lu and Yansong Tang.<br>
        <i>Preprint</i><br>
        <br>
        We propose a dynamic Gaussian Splatting method named ManiGaussian for multi-task robotic manipulation, which mines scene dynamics via future scene reconstruction.
    </div> 
</div>

<!--Paper 2-->
<div class="row">   
    <div class="column" style="float:left;width:25%"> 
     	  <img src="../images/nae.png">
    </div>
    <div class="column" style="float:left;width:75%">    
        <b>Narrative Action Evaluation with Prompt-Guided Multimodal Interaction</b><br>
        <b>Shiyi Zhang*</b>, Sule Bai*, Guangyi Chen, Lei Chen, Jiwen Lu, Junle Wang, Yansong Tang<br>
         <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</i>, 2024<br>
        <br>
        We investigate a new problem called narrative action evaluation (NAE) and propose a prompt-guided multimodal interaction framework.
    </div> 
</div>

<!--Paper 3-->
<div class="row">   
    <div class="column" style="float:left;width:25%"> 
     	  <img src="../images/logo.png">
    </div>
    <div class="column" style="float:left;width:75%">    
        <b>LOGO: A Long-Form Video Dataset for Group Action Quality Assessment</b><br>
        <b>Shiyi Zhang</b>, Wenxun Dai, Sujia Wang, Xiangwei Shen, Jiwen Lu, Jie Zhou, Yansong Tang<br>
         <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</i>, 2023<br>
        <br>
        LOGO is a new multi-person long-form video dataset for action quality assessment.
    </div> 
</div>

